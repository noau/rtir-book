# Brute Force Ray-Tracing

This chapter looks a bit long, but in we'll discuss and implement the core pipeline of a brute force ray tracer, with the only geometry supported - sphere, without materials. However, this will form the very basic of our renderer, and all following chapters is to add more functionalities to it, without big changes in the core logic.

## Core Rendering Flow

![Renderer Main Flow](/images/renderer-flow.svg){#fig-main-flow fig-align="center"}

As shown in @fig-main-flow, there're generally three phases. After we start the renderer, we'll first load the scene into the memory, then we'll configure some parameters specified to set the renderer to a good state for rendering. After all preparation, we'll finally start rendering the scene to our film, and save the result.

### Scene

Ideally, we should have a file format to describe the scene. The scene contains not only all the geometries with their location, materials, etc. Actually, the so called scene here contains also other necessary things like HDRIs, camera parameters, and rendering algorithms.

However, to start more simply, we just use a simple function instead. This function should construct the scene when called, and just return it to the caller so we can use it for later process. It should also accept a parameter specifying what scene we want, because we may add support for scene description file in Part 3, and we can leave those hard-coded scenes as sample scenes.

### Renderer Configuration

There're two places where renderer configurations come from, one is the scene description file and the other one is cli parameters. However, at this starting stage, we have none of them. So by now all parameters will be specified in the scene construction function, all hard-coded.

This phase is designed here so that we can override the parameters in scene description with cli parameters. It should also resolve all conflicting/impossible parameters to avoid rendering errors. Things like progress bar and previewer should also be set up here. Note that **not all** parameters would/should be configured here. For example, the log control will be taken into consideration at the very early stage, even before we're trying to parse cli parameters later when we start to add a usable interface.

### Rendering

Absolutely the rendering phase is core of most renderer, so we'll break it down into a lot pieces, and explain them in the whole next section.

## Rendering in Pieces

::: callout-important
## Figures

There should be a lot figures explaining those rendering process. They'll be added later at the end of first draft.
:::

### Pixel Rendering

Let's first focus on one pixel on the image. Like the sketch of our virtual camera, we can try to get the color at this pixel by sending a ray from the camera center through the pixel, and calculate the color seen along this ray. However, the pixel is a plane, where our ray should have only one intersection point with it. This means that by this way, we can only get the color of one point in the pixel, which sounds bad.

How can we get the color of the whole pixel? Well, we cannot get it actually, if we use only one ray. And unless if you use infinite rays, you can never specify all colors on this pixel! However, the pixel can have only one pixel, so we can just use the color of one ray, right?

Well. As shown in the figure, it is possible that one pixel could actually contains several different colors, especially at the edge of the geometry. If we just we one ray to test the color, we can only have one color, either one side or another side. This sound even worse! What we're really expecting should be some mixture of those colors, that makes our eyes satisfied. Intuitively our solution is to send a lot rays whose intersection points with the film should all lies in the pixel, and we can use the means of all colors from every ray, so that we can have the mixed color. Every ray is called a sample, and the amount of rays we use for one pixel is call samples per pixel, or spp, which is almost the most important parameter of a pure ray tracer.

### Ray Rendering

To figure out how we can get the color along a ray, there're several questions to be answered. Fisrt, we all know color is a property of light, and in physical world, light come from the outside of the camera, through lens, and finally hit the film, but why do we send ray starting inside the camera?

Well, in physics, as we've learned, studies of light at macroscopic level are classified into two categories - geometric optics and wave optics. What we've assumed as a preliminary in this renderer, is that, lights in our world follows geometric optical law. So we can say that light travels in straight lines, and the light path is reversible. This means that we can track the light reversely from the end of the light path, i.e., we can send ray inside the camera and finally reach the light source!

But this leads to another question - how would we simulate light source? Accurate simulation of light sources requires some more works on physical property, and this would be achieved in Part 2. For now, we're going to use a simpler method, and this requires us to discuss about the interaction when the ray hits a geometry object.

When a ray hit an object, we'll calculate some related properties related to this intersection point, like its location in 3D space and on the ray, and the surface normal at this point. Those properties along with some useful information like the material at this point will be recorded, in my `Rust` implement, in a `Intersection` struct.

Then we'll calculate the interaction of the light and the surface at this point. When the light hits the surface, possibly some energy of the light will be absorbed. Left energy would spread further. Most of time those energy will be scatter into other rays, so we'll do the same **Ray Rendering** process for them, recursively. Some materials allows transmission. Under that scenario, some part of energy will become transmission ray, and we also apply **Ray Rendering** process to it. After calculating all those generated rays, we'll add all contributions of them together, and get the final result of this ray. Now the light source comes. Currently in this simple ray tracer, we assume that one material could either emit light, or scatter light, without mixed capabilities to both emit and scatter light. So a ray will have no energy carried unless it encounters a light source, so we can say that a light source is where we can directly obtain the color of the ray without the need to scatter it, and that's why our renderer is not physically plausible now.

Some trivial light like emitting material will be implemented later in this part.

### Image Rendering

We now have the capability to render a pixel. So intuitively, we can render pixels in a image one by one, and that's totally usable. However, this process could be slow and time-consuming, especially when the scene is complex or the result image has high resolution. There're many algorithms trying to improve the performance, like use efficient data structure to accelerate the calculation of ray-scene intersection, that will be discussed soon. However, there's a simple and intuitive way to accelerate the rendering process, i.e., render the image in a parallel fashion. The most direct thought is to calculate every pixel in a thread, but that means we'll need millions of threads, that is impossible. And the performance cost of frequently create and switch between threads is also huge.

Our choice is to render the image in tiles. By splitting the whole image into to some small tiles, like $32 \times 32$, we can both have the performance improvement of parallel rendering, and avoid the cost of too much thread operation.

## Implementation Design

### Ray & Camera

To start, we'll need a type representing ray. The math formula of ray could be written in a parametric form, $p = \mathbf o + t \mathbf d$, where $\mathbf o$ is the origin of the ray, and $\mathbf d$ points at the direction. If $\mathbf d$ is a unit vector, the parameter $t$ has the property that $|\mathbf d|$ is the length of the ray. There's no need to store `t` within the ray since we can calculate it on demand, and the ray type is trivial to implement and use. Therefore a method for evalute the point at parameter `t` is also needed.

Then we will also need a type representing the virtual camera. The ray starts at the center of our camera, and its direction is determined by sample point of the pixel on the film plane. So what we only need for virtual camera type is those two things.

The most intuitive way to generate a sample way is described as above, the ray crossing the center and the sample point. However, there're multiple different way to choose the two points. When you generate in the way above, you'll get an effect of so called perspective projection. But there's also orthogonal projection. There's also a trick to implement the caustics, that is to randomly choose a start point in a small area around the center. Another trick about this is how to choose the sample point on the film for a pixel. We can choose a random point around within the pixel, but we can also divide the pixel by the number of sample count, and choose one pixel for one small area, called jittering, or stratified samples. The theory will be introduced later, but it usually provides a better result. So we need to include two parameters describing those effects.

However, besides those, we'll also need some other parameters to determine the looking direction and field of view of the camera. This involves how to determine the local coordinate of the camera. For this local coordinate, we choose the positive $x$ direction pointing the right of the camera, and the positive $y$ direction up. Following right hand convention, the positive $z$ direction will point backwards, in the opposite direction of our ray. When constructing a camera, we require the user to provide the position $\mathbf p$, the $\mathbf u$ direction, i.e., the up direction and a point, $\mathbf t$, that the camera is looking at, where $\mathbf p$ and $\mathbf t$ must be different and the $\mathbf u$ direction mustn't lies on the line of . Then we'll calculate formed by $\mathbf p$ and $\mathbf t$. Then we can construct the coordinate by following steps: the positive $\mathbf z$ direction is simply got by $- \frac{(\mathbf t - \mathbf p)}{|\mathbf t - \mathbf p|}$, i.e., the normalized opposite direction of looking direction. Then we can have the positive $\mathbf x$ direction by a cross product, $\frac{\mathbf u \times \mathbf z}{|\mathbf u \times \mathbf z|}$. A final cross product gives us the correct $\mathbf y$ direction, $\mathbf z \times \mathbf x$, without the need to normalize since it is already.

Then some other parameters are need to determine the film. The first thing is the resolution, which will be used to construct a corresponding film with same size. Another one is the field of view of our camera. This involves a design decision. There's a probability that you can specify a camera with both horizontal and vertical FOV set at 80 degree, but output the image in a different aspect ratio, like the commonly used $1280 \times 720$. However I didn't choose this, if you're interested in this, see [Exercise @sec-p1c4-qcfp-irr-ratio]. There's also a problem about the relative positive position of camera center and the film. We assume that originally the camera center will be projected to the center of the film, but an offset could be passed to adjust it along x and/or y axis (under the camera space).

So the parameters needed are a film reference (for resolution), and a vertical FOV. Using those, we'll construct a virtual film for our camera that helps us generate rays. We assume the height out our virtual film is always $2$ meters. The length from the center to the virtual film could be calculated with vertical FOV, as $d = \frac{1}{tan \frac{FOV}{2}} = cot \frac{FOV}{2}$. The width of virtual film could be calculated with aspect ratio, i.e., $width = 2 \frac{film\_width}{film\_height}$. We'll let the camera generate a sample for a pixel by passing its index $(i, j)$, so we need a way to calculate the area of that pixel quickly. This is actually easy by storing the top left position of the top left pixel, $\mathbf o$, and the size of the pixel along positive $x$ and negative $y$ direction, $\mathbf{dx}$ and $\mathbf{dy}$, so that the top left corner of pixel $(i, j)$ is thus $\mathbf o + i\mathbf{dx} + j \mathbf{dy}$. They can be calculated as following:

$$
\begin{aligned}
\mathbf o &= \mathbf p - d \mathbf z - \frac{width}{2} \mathbf x - \frac{height}{2} \mathbf y = \mathbf p - d \mathbf z - \frac{width}{2} \mathbf x - \mathbf y \\
\mathbf {dx} &= width \frac{\mathbf x}{film\_width} \\
\mathbf {dy} &= - height \frac{\mathbf y}{film\_height} = - \frac{2 \mathbf y}{film\_height}
\end{aligned}
$$

So when a user call the camera to generate a sample ray for pixel $(i, j)$ as the $k$th sample, it should first calculate the top left corner of that pixel by above formulas, and pass it with $\mathbf{dx}$, $\mathbf{dy}$ and $k$ to the `SamplePointGenerator` for a sample point. Then the sample point (used for caustics mentions above) with some other necessary information (if any) would be passed to `SampleStartGenerator` for a start point of the ray, and finally give out the ray defined by this two point.

### Geometry Object & Sphere

First, geometry objects should be able to do intersection test with a given ray. Information about the intersection should be returned and there's csould also be no intersection with that ray. If there is, necessary information that should be returned and remained for later rendering usage includes:

-   Surface normal and is front
-   Intersection point and the parameter along the ray
-   Reference to material of the object

Those are the very basic information. Later we'll add more like the uv coordinate of the intersection point on the geometry. The surface normal should point opposite from the ray, i.e., when the ray hit the geometry from outside, the surface should points outwards, and vice versa. The `is front` just indicates if the ray hit the geometry from outside or not. This could be handled by the constructor of the intersection type, with the ray direction, or by the outer caller that constructs the record directly. If given an normal points outwards, and a ray direction, we can just invert the direction of the normal if the dot product between outward normal and the ray direction is positive, and thus the normal points inward. Otherwise we now the normal should remain outwards.

We also require all geometry objects calculate a bounding box to be able to do intersection test more effectively, and we support only AABB for simplicity now. See [PBRT 3.7 Bounding Boxes](https://pbr-book.org/4ed/Geometry_and_Transformations/Bounding_Boxes.html) and [PBRT 6.1.2 Ray-Bounds Intersections](https://pbr-book.org/4ed/Shapes/Basic_Shape_Interface#RayndashBoundsIntersections) for detailed explanations. We also assume that all geometry objects are defined in their local coordinate spaces, centered at the origin without rotation and scaling. Those will be dealt generally in next chapter. Given the AABB, we can have a method to quickly check whether a ray miss an object - if a ray cannot hit even the AABB of the primitive, it can never hit the object itself. We'll provide a general method to test in this way that only gives a boolean value representing whether a ray hits the AABB of the geometry. Another similar method is also required, to calculate whether the ray hits the geometry itself, without further calculation about things like normals.

The first and most widely used geometry primitives in the part 1 are spheres. A sphere could be defined by its center and radius. Since we require objects to be defined in local coordinate system, the center of the sphere is always at the origin, and the only parameter related to its geometric properties is its radius.

So, given the radius $r$ of a sphere, we can write the equation of the sphere, thus every point $\mathbf p$ on the sphere satisfies $|\mathbf p| = r$. Combined with the ray equation, $\mathbf p = \mathbf o + t \mathbf d$, we can have a new equation: $|\mathbf o|^2 + 2 t \mathbf o \cdot \mathbf d + t^2 |\mathbf d| ^ 2 - r^2 = 0$. Using the quadratic formula, wen can sole the result of $t$ easily, thus

$$
\begin{aligned}
t &= \frac{-2 \mathbf o \cdot \mathbf d \pm \sqrt{(2 \mathbf o \cdot \mathbf d)^2 - 4 |\mathbf d| ^ 2 (|\mathbf o|^2 - r^2)}}{2|\mathbf d|^2} \\
&= \frac{-\mathbf o \cdot \mathbf d \pm \sqrt{(\mathbf o \cdot \mathbf d)^2 - |\mathbf d| ^ 2 (|\mathbf o|^2 - r^2)}}{|\mathbf d|^2}
\end{aligned}
$$

Since the $\mathbf d$ is always normalized, the value of $|\mathbf d|$ is always $1$, thus the formula could be written as

$$
t = - \mathbf o \cdot \mathbf d \pm \sqrt{(\mathbf o \cdot \mathbf d)^2 - (|\mathbf o|^2 - r^2)}
$$

We might have 0, 1 or 2 solutions of $t$, but not all of them are valid. As stated above, the $t$ here is related to the length of the ray, so it must be positive, otherwise, the camera could see objects behind itself. We'll also give $t$ a upper limit, thus the camera can only see objects within a finite rage. However, it might be a little surprising that we'll also give $t$ a lower limit - a small value greater than but not equal to 0. This is due to precision error introduced by float number representation in computers. And it's very intuitive that we should take the smaller solution when there're two different valid solutions the ray hits the nearest point along its way. After calculated $t$, we can have $\mathbf p$ very easily by substitute it in the ray equation, and the normal on the sphere at the point is also equal to $\frac{\mathbf p}{|\mathbf p|}$, given that our sphere is centered on the origin.

### Material & Phong Shading

### Scene Loader & Scene

### Main Method

## Implementation

### Ray

The ray type is trivial to implement. One thing to be noted is that we hope the direction of a ray should always be unit vector. We'll achieve this by a small trick: wrapper pattern as smart pointer. This trick is used on `Vector` instead of `Ray`, as following:

``` rust
#[derive(Debug, Copy, Clone)]// <1>
pub struct UnitVector3(Vec3); 

impl UnitVector3 {
    pub fn new(v: Vec3) -> Self {
        Self(v.normalize())
    }
}

impl Deref for UnitVector3 { // <2>
    type Target = Vec3;

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl DerefMut for UnitVector3 { // <3>
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}
```

1.  Extra traits like `From<Vec3>` and `Default` are omitted.
2.  What actually used in my code is to implement those two trait automatically using `derive_more` crate, with `#[derive(Deref, DerefMut)]`. This one here is only for demostration.
3.  `DerefMut` requires `Deref` to be implemented, and use the same `Target` type as `Deref`.

::: callout-note
## Why not auto implement operator traits?

This is because we cannot determine the result type for the operator traits. When you do calculations between `UnitVector`s and normal `Vec3`s, or `f32` values, you can not know whether the result is `UnitVector` or `Vec3`. Though you can always set the result type to `Vec3`, this isn't what the compiler should promise - it's too strong that not everyone wants it.
:::

And we can test its correctness:

``` rust
#[test]
fn test_unit_vector() {
    let v = Vec3::new(2.0, 0.0, 0.0);
    let unit_v = UnitVector3::new(v);
    assert_eq!(unit_v.x, 1.0);
    assert_eq!(unit_v.y, 0.0);
    assert_eq!(unit_v.z, 0.0);
    assert_eq!(unit_v.length(), 1.0);
}
```

So the `Ray` could be like this:

``` rust
#[derive(Debug, Clone, Copy, Default)]
pub struct Ray {
    pub origin: Vec3,
    pub direction: UnitVector3,
}

impl Ray {
    pub fn new(origin: Vec3, direction: UnitVector3) -> Self {
        Self { origin, direction }
    }

    pub fn evaluate(&self, t: f32) -> Vec3 {
        self.origin + self.direction.0 * t // <1>
    }
}
```

1.  Note that here we still need to use `self.direction.0` instead of `self.direction`. Although the method `mul` defined in `Mul` was brought into `UnitVector3`(throught `Deref` and `DerefMut`), the type `UnitVector3` it self cannot be used in operator since `Mul` is not implemented for it.

Write a test for it. This should be very easy to test.

### Material

### Phong Shading

### Primitive

We'll first define the intersection record:

``` rust
pub struct Intersection {
    pub ray_length: f32,
    pub point: Vec3,
    pub normal: UnitVector3,
    pub is_front: bool,
}

impl Intersection {
    pub fn new(
        ray_length: f32,
        point: Vec3,
        outwards_normal: UnitVector3,
        ray_direction: UnitVector3,
    ) -> Self {
        let (normal, is_front) = Self::adjust_normal(outwards_normal, ray_direction);

        Self {
            ray_length,
            point,
            normal,
            is_front,
        }
    }

    fn adjust_normal(
        outwards_normal: UnitVector3,
        ray_direction: UnitVector3,
    ) -> (UnitVector3, bool) {
        if ray_direction.dot(outwards_normal.0) > 0.0 {
            (outwards_normal.neg().into(), false)
        } else {
            (outwards_normal, true)
        }
    }
}
```

Write unit tests for the `Intersection::adjust_normal` method. Don't worry, `Rust` supports unit tests of private methods.

Then we're going to define the axis aligned bounding box following [PBRT 3.7 Bounding Boxes](https://pbr-book.org/4ed/Geometry_and_Transformations/Bounding_Boxes.html):

``` rust
#[derive(Debug, Copy, Clone, PartialEq)]
pub struct AABB {
    pub min: Vec3,
    pub max: Vec3,
}
```

Here we only defines the so called `Bounds3f`, for we'll use it later only. Other bounds are not used so there's no need to define and implement them, which also leads to simplicity. All functions defined in *PBRT* could also be implemented trivially in our `Rust` version. Useful traits like `Index` and `IndexMut` are implemented too. And since we're using 3rd party linear algebra library, some code could even be simpler. For example, when implementing the `Inside` function as `contains` method, instead of compare x, y, and z components individually, we can just compare the whole vector:

``` rust
impl AABB {
    // ...

    pub fn contains(&self, p: Vec3) -> bool {
        p.cmpge(self.min).bitand(p.cmple(self.max)).all()
    }

    // ...
}
```

Which not only simplifies the code, but also make it more readable and clear. Try to find more useful methods provided by `glam` and use as shorter and clear code as you can. Another thing to be noted is that for the `bouunding_sphere` method, instead return by parameter, we return another struct called `SphereBound` that contains the center and radius of the sphere, like so:

``` rust
pub struct SphereBound {
    pub center: Vec3,
    pub radius: f32,
}

impl AABB {
    // ...

    pub fn bounding_sphere(&self) -> SphereBound {
        let center = (self.min + self.max) / 2.0;
        SphereBound {
            center,
            radius: center.distance(self.max),
        }
    }

    // ...
}
```

The only troublesome is to write tests for all of them, but it's worthy.

Now we have to consider about the ray-AABB intersection test, which also follows the *PBRT*'s approach:

``` rust
pub const MACHINE_EPSILON: f32 = f32::EPSILON * 0.5; // <1>

pub fn gamma(x: f32) -> f32 {
    x * MACHINE_EPSILON / (1.0 - x * MACHINE_EPSILON)
}

impl AABB {
    // ...

    pub fn hit(&self, ray: Ray) -> Option<(f32, f32)> {
        let mut t0 = 0.0;
        let mut t1 = f32::MAX;
        for i in 0..3 {
            let inv_d = 1.0 / ray.direction[i];
            let mut t_near = (self.min[i] - ray.origin[i]) * inv_d;
            let mut t_far = (self.max[i] - ray.origin[i]) * inv_d;
            if t_near > t_far {
                std::mem::swap(&mut t_near, &mut t_far);
            }

            t_far *= 1.0 + 2.0 * gamma(3.0); // <2>

            t0 = t_near.max(t0);
            t1 = t_far.min(t1);
            if t0 > t1 {
                return None;
            }
        }

        Some((t0, t1))
    }

    pub fn intersect(&self, ray: Ray, rt_max: f32, inv_d: Vec3, dir_neg: [usize; 3]) -> bool {
        let mut t_min = (self[dir_neg[0]].x - ray.origin.x) * inv_d.x;
        let mut t_max = (self[1 - dir_neg[0]].x - ray.origin.x) * inv_d.x;
        let ty_min = (self[dir_neg[1]].y - ray.origin.y) * inv_d.y;
        let ty_max = (self[1 - dir_neg[1]].x - ray.origin.y) * inv_d.y;

        if t_min > ty_max || ty_min > t_max {
            return false;
        }

        t_min = ty_min.max(t_min);
        t_max = ty_max.min(t_max);

        let tz_min = (self[dir_neg[2]].z - ray.origin.z) * inv_d.z;
        let tz_max = (self[1 - dir_neg[2]].z - ray.origin.z) * inv_d.z;

        if t_min > tz_max || tz_min > t_max {
            return false;
        }

        t_min = tz_min.max(t_min);
        t_max = tz_max.min(t_max);

        t_min < rt_max && t_max > 0.0
    }

    // ...
}
```

1.  This is a `const` value more like the `static constexpr` in `C++` than `macro`s.\
2.  It's better to use an additional trait to define and implement the `gamma` as a method instead of a function. However, the name `gamma` is already used by an unstable feature of `Rust`, so we just use a function instead.

Those're basically plain translation from the *PBRT*'s `C++` code. The original names for this two functions are `Intersect` and `IntersectP`, but I prefer to use `hit` and `intersect` instead. Choose some bounds and rays carefully and test them.

Finally we can define the interface of geometry objects, as a trait, like so:

``` rust
pub struct SurfaceHit {
    pub normal: UnitVector3,
    pub is_front: bool,
    pub ray_length: f32,
    pub hit_position: Vec3,
}

pub trait Shape {
    fn hit(&self, ray: Ray) -> Option<SurfaceHit>;
    fn intersect(&self, ray: Ray, rt_max: f32) -> bool;
    fn aabb(&self) -> AABB;
}
```

### Sphere & `enum_delegate`

Thus the `Sphere` definition and implementation is trivial:

``` rust
pub struct Sphere {
    pub radius: f32,
    pub aabb: AABB,
}

impl Sphere {
    pub fn new(radius: f32) -> Self {
        Self {
            radius,
            aabb: AABB::from_two_points(Vec3::splat(-radius), Vec3::splat(radius)),
        }
    }
}

impl Shape for Sphere {
    fn hit(&self, ray: Ray, rt_max: f32) -> Option<SurfaceHit> {
        let od = ray.direction.dot(ray.origin);
        let delta = od * od - ray.origin.dot(ray.origin) + self.radius * self.radius;
        if delta < 0.0 { return None; }

        let delta_sqrt = delta.sqrt();
        let t1 = -od - delta_sqrt;
        let t2 = -od + delta_sqrt;

        if t2 <= 0.0 || t1 > rt_max || (t1 <= 0.0 && t2 > rt_max) { return None; }

        let t = if t1 > 0.0 { t1 } else { t2 };

        let hit_position = ray.origin + *ray.direction * t;

        Some(SurfaceHit {
            normal: hit_position.into(),
            is_front: ray.direction.dot(hit_position) < 0.0,
            ray_length: t,
            hit_position,
        })
    }

    fn intersect(&self, ray: Ray, rt_max: f32) -> bool {
        let od = ray.direction.dot(ray.origin);
        let delta = od * od - ray.origin.dot(ray.origin) + self.radius * self.radius;
        if delta < 0.0 { return false; }

        let delta_sqrt = delta.sqrt();
        let t1 = -od - delta_sqrt;
        let t2 = -od + delta_sqrt;

        t2 > 0.0 && t1 <= rt_max && (t1 > 0.0 || t2 <= rt_max)
    }

    fn aabb(&self) -> AABB {
        self.aabb
    }
}
```

This implementation may not be well-optimized, but it's intuitive and enough for us.

Currently we're using `trait` to define the common behaviour of geometry objects, and we have only one implementation - `Sphere`. We can, indeed, just use the `Sphere` struct everywhere in our code, but later when we add more shapes like planes and boxes, this cannot work. So we need some sort of polymorphism that allows us to have multiple implementations.

Generally we have static and dynamic dispatch in `Rust`. The static dispatch is done at the compile stage, via generics, and the dynamic dispatch is done at the runtime. However, there will be multiple objects in one scene, and most of the time it's not possible they're same type. The container we store all objects, like `Vec`, must be heterogeneous. So must we use dynamic dispatch through `dyn Shape`? Well, actually not so.

When the parameters are given outside our own crate, we actually have only the choice. But in our case, all geometry types are in our own crate, and is known to us. What we have is a known, finite set of `Shape` implementations, and thus, we can use a special way - the `enum`.

The idea is simple. We can just define a `enum` with each variant corresponding to a `Shape` implementation. Then we can just use the `enum` as the object type, which is homogeneous to the type system, but heterogeneous in the implementation. We can implement the `trait` for both the `enum` and the actuall `Shape` implementation types, and simply delegate the implementation of the `trait` for the `enum` to the imeplementation of each variant, the actuall `Shape` implementation.

And you and find the obvious boilerplate here, so the `Rust` community has already given a lot of help to eliminate the boilerplate, through `macro`s. What we're going to use here, is the `enum_delegate` crate.

It's easy to use. Just add `#[enum_delegate::register]` to the trait and `#[enum_delegate::implement(<trait>)]` to the `enum`, like so:

``` rust
#[enum_delegate::register]
pub trait Shape {
    // ...
}

#[enum_delegate::implement(Shape)]
pub enum Primitive {
    Sphere(Sphere) // <1>
}
```
1. This looks weird - the token `Sphere` repeated twice! But don't worry. It's fine. They live in different name spaces. The first `Sphere` is an enum variant and the second one is the struct we define above. And this is not the benefincal of `enum_delegate`. It's just something that vanilla `Rust`'s `enum` accepts.

Besides the relation we've stated above, `From` and `TryInto` traits between the `enum` and `struct`s are also auto-implemented.

::: callout-note
## Why not `enum_dispatch`?

While `enum_dispatch` is maturer, some IDEs - I mean IDEs from JetBrains - cannot index the codes generated by it correctly, but it seems codes generated by `enum_delegate` are acceptable.
:::

### Scene

### Camera

Film will not be included in the camera. Actually, this camera is just a virtual camera as a rendering helper. Following the discussion above, the camera could just be a simple struct holding the camera parameters, like:

``` rust
#[derive(Debug, Clone)]
pub struct Camera {
    pub position: Vec3,
    pub coordinate: Mat3,
    pub spp: NonZeroUsize,
    spp_jitter: (usize, usize),
    o: Vec3,
    d: (Vec3, Vec3),
    d_jitter: (Vec3, Vec3),
    pub jittering: bool,
    pub orthographic: bool,
    dist: f32,
}
```

The only things needing explanation is the two more parameters, `spp_jitter` and `jittering`.

We use $\mathbf o + i\mathbf{dx} + j \mathbf{dy}$ for the location of a pixel, but how can we know the exact area of the sample point when using jittering? The answer is that we need to know how to split one pixel to exact number of smaller areas as spp. I choose to choose two factors of the spp count, $x$ and $y$, where $x \times y = spp$ and $|x-y|$ is minimal. This could lead to a more even distribution of sample points within the pixel, and we can treat them as a smaller 2D array, accessing each one with a single index, i.e., the sample index $k$ in the code in a row-major style. And we can use a similar formula for this that the top left corner of this small area $\mathbf o + i\mathbf{dx} + j \mathbf{dy} + m \frac{\mathbf{d_x}}{x} + n \frac{\mathbf{d_y}}{x}$, where `d_jitter` is the new `m` and `n`, and ${\mathbf{d_x}}{x}$ and ${\mathbf{d_y}}{x}$ are the new `dx` and `dy` for the smaller pixel. Those will also be pre-calucated during the construction of the camera. So the constructor is just:

``` rust
#[allow(clippy::too_many_arguments)] // <1>
pub fn new(
    position: Vec3,
    look_at: Vec3,
    up: Vec3,
    film: &Film, // <2>
    v_fov: f32,
    offset: Vec2,
    spp: NonZeroUsize,
    jittering: bool,
    orthographic: bool,
) -> Self {
    // Construct camera space coordinate
    let z = (position - look_at).normalize();
    let x = up.cross(z).normalize();
    let y = z.cross(x);
    let coordinate = Mat3::from_cols(x, y, z);

    // Construct virtual film
    let dist = 1.0 / (v_fov / 2.0).tan();
    let width = 2.0 * film.shape()[0] as f32 / film.shape()[1] as f32;

    let o = position - dist * z + (width / 2.0 * x) - y + offset.x * x + offset.y * y;
    let d = (width * x / film.shape()[0] as f32, -2.0 * y / film.shape()[1] as f32);

    // Break spp into x and y that x * y = spp and both x and y are as close as possible
    let spp_jitter = find_closest_factors(spp.get());
    let d_jitter = (d.0 / spp_jitter.0 as f32, d.1 / spp_jitter.1 as f32);

    Self {
        position,
        coordinate,
        spp,
        spp_jitter,
        o,
        d,
        d_jitter,
        jittering,
        orthographic,
        dist,
    }
}
```

1.  This function contains 9 parameters - too many that clippy will complains about it, so we need to explicitly accept it for this function. Don't allow it globally, it's still a good lint message, those not good for this one.
2.  The `Film` is used since we need to know about the resolution of the film, but it don't actually need to reference it it later or own it.

As warned by clippy, it contains too much parameters. Create a build for it and provide a default settings for camera. This should be easy and it's left as an exercise. Don't forget to provide a `builder()` function for `Camera` that create a default builder so we don't need to remember the name of the builder.

Then we can generate the ray in two steps, as also described above:

``` rust
pub fn generate_ray(&self, (i, j): (usize, usize), k: usize) -> Ray {
    let p = self.o + i as f32 * self.d.0 + j as f32 * self.d.1;
    let sample_point = self.generate_sample_point(p, k);
    let start_point = self.generate_start_point(sample_point);
    Ray::from_start_end(start_point, sample_point)
}

fn generate_sample_point(&self, p: Vec3, k: usize) -> Vec3 {
    let x: f32 = rand::random();
    let y: f32 = rand::random();
    let (o, dx, dy) = if self.jittering {
        (
            // Treat the small jittering grid as 2D array, with row-major index
            (k / self.spp_jitter.1) as f32 * self.d_jitter.0
                + (k % self.spp_jitter.1) as f32 * self.d_jitter.1,
            self.d_jitter.0,
            self.d_jitter.1,
        )
    } else {
        (Vec3::splat(0.0), self.d.0, self.d.1)
    };
    p + o + x * dx + y * dy
}

fn generate_start_point(&self, sample: Vec3) -> Vec3 {
    if self.orthographic {
        sample + self.coordinate.z_axis * self.dist
    } else {
        self.position
    }
}
```

## Questions, Challenges & Future Possibilities {#sec-p1c4-qcfp}

### Wrapper Pattern for `Film` Implementation {#sec-p1c4-qcfp-wrapper-film}

As we've seen in @sec-film, we've chosen to implement `Film` as a type alias to `Array2`. However, we can do the same thing with the wrapper pattern and `Deref` trick introduced in this chapter, with even better result. Try it in your code.

### Irrelated Film and Virtual Film Aspect Ratio {#sec-p1c4-qcfp-irr-ratio}

This is pretty trivial. Just provide a new horizontal FOV parameter and modify the constructor of camera accordingly.

### Logic in `SamplePointGenerator` and `SampleStartGenerator` {#sec-p1c4-qcfp-trait-generator}

We use two `bool` parameters for the customizing ray generting, but you can actually extract them into new type, with even new traits, as described simply in the question header. This is actually easy and intuitive.